{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "roObUYgPgUSf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYrInhLbSlzW",
        "outputId": "dfef4f47-b3bb-49bf-b1b2-dc6cae63fac8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Imports\n",
        "# utilities\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# important libraries\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.io import output_file, show, output_notebook\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from spacy.util import compounding\n",
        "from spacy.util import minibatch\n",
        "from spacy import displacy\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# sklearn\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZUw7pWqf3mA",
        "outputId": "cb05d894-0cde-46a4-dd5a-c8235283fce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/warData\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/warData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "omyUirwO66tH"
      },
      "outputs": [],
      "source": [
        "csv_collection = []\n",
        "for dirname, _, filenames in os.walk('/content/warData/MyDrive/archive/UkraineWar'):\n",
        "    for filename in filenames:\n",
        "        fullpath= os.path.join(dirname, filename)\n",
        "        csv_collection.append(fullpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9P6DS-HSeJj",
        "outputId": "1c89c65b-4f50-4538-e412-6aea36fa8bf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(csv_collection.pop(),compression = 'gzip', index_col=0)\n",
        "for data in csv_collection:\n",
        "    try:\n",
        "        tmp = pd.read_csv(data, compression = 'gzip', index_col=0)\n",
        "    except:\n",
        "            tmp = pd.read_csv(data, index_col = 0)\n",
        "            df = pd.concat([df, tmp], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCV4vU9DX0u_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import urllib\n",
        "import csv\n",
        "from scipy.special import softmax\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-mHI6KBVkkI"
      },
      "outputs": [],
      "source": [
        "sentiment_df = df[['tweetid', 'text', 'hashtags', 'language']] \n",
        "sentiment_df = sentiment_df.loc[sentiment_df['language'] == 'en'].reset_index(drop=True)  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "sentiment_df['hashtags'] = sentiment_df.hashtags.map(lambda x: [i['text'] for i in eval(x)]) \n",
        "sentiment_df['text'] = sentiment_df['text'].apply(preprocess)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30Bfu72EYujj"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(device)\n",
        "\n",
        "task='sentiment'\n",
        "labels=[]\n",
        "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
        "with urllib.request.urlopen(mapping_link) as f:\n",
        "    html = f.read().decode('utf-8').split(\"\\n\")\n",
        "    csvreader = csv.reader(html, delimiter='\\t')\n",
        "labels = [row[1] for row in csvreader if len(row) > 1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAie_EPMovrW"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "from tqdm import tqdm\n",
        "\n",
        "BATCH_SIZE = 100 # number of tweets in a batch that will be passed into tokenizer\n",
        "\n",
        "scores_all = np.empty((0,len(labels)))\n",
        "text_all = sentiment_df['text'].to_list()\n",
        "with torch.no_grad():\n",
        "    for start_idx in range(0, n, BATCH_SIZE):\n",
        "        end_idx = min(start_idx+BATCH_SIZE, n)\n",
        "        encoded_input = tokenizer(text_all[start_idx:end_idx], return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        output = model(**encoded_input)\n",
        "        scores = output[0].detach().cpu().numpy()\n",
        "        scores = softmax(scores, axis=1)\n",
        "        scores_all = np.concatenate((scores_all, scores), axis=0)\n",
        "        del encoded_input, output, scores\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "\n",
        "sentiment_df[labels] = pd.DataFrame(scores_all, columns=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_CXQPFhpypx"
      },
      "outputs": [],
      "source": [
        "sentiment_df.to_csv(\"./Tweets_Sentiment_Analysis_RoBERTa_Raw_Values.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
